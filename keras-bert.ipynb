{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "nteract": {
      "version": "0.12.3"
    },
    "colab": {
      "name": "keras-bert.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR4k9BSqlkhy",
        "colab_type": "text"
      },
      "source": [
        "### Install Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5gB0XAUbSOI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "ebb07c84-3db2-435f-938e-dab8f8589101"
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 27.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 34.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 39.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 42.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 46.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 30.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvpUjdYamDH9",
        "colab_type": "text"
      },
      "source": [
        "### Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba-Y9DPWbQmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from bert.tokenization import FullTokenizer\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Params for bert model and tokenization\n",
        "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "max_seq_length = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pvzqwU2mHI3",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alU0YDHsbQmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "4979b2f0-4fa3-42f7-eaa9-0529a6e7b5ae"
      },
      "source": [
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "\n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "# Reduce logging output.\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "train_df, test_df = download_and_load_datasets()\n",
        "train_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie brought tears to my eyes; John Robe...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I am a big fan of Ludlum's work, and of the Co...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can a movie be both controversial and gent...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OMG this is one of the worst films iv ever see...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Although I can understand the bad things someo...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence sentiment  polarity\n",
              "0  This movie brought tears to my eyes; John Robe...        10         1\n",
              "1  I am a big fan of Ludlum's work, and of the Co...         3         0\n",
              "2  How can a movie be both controversial and gent...         8         1\n",
              "3  OMG this is one of the worst films iv ever see...         1         0\n",
              "4  Although I can understand the bad things someo...         7         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBAOsysNmeet",
        "colab_type": "text"
      },
      "source": [
        "### Data Preaparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L7zkAoUbQmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create datasets (Only take up to max_seq_length words for memory)\n",
        "train_text = train_df['sentence'].tolist()\n",
        "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "train_label = train_df['polarity'].tolist()\n",
        "\n",
        "test_text = test_df['sentence'].tolist()\n",
        "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "test_label = test_df['polarity'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga_QmZSEw4xH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "028e131b-2c60-472d-ad66-9c46e3ad1bb3"
      },
      "source": [
        "train_text[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['This movie brought tears to my eyes; John Roberts really knew how to get to viewers\\' hearts, directing this wonderful picture where life is viewed through the mind and heart of Paulie. We discover from time to time, with the help of sensitive and talented directors like John, that even small creatures like Paulie have a heart. I just couldn\\'t stop my tears, even though the film has a happy end. This is great, after thousands of films I saw through my life, \"Paulie\" really touched me deeply. This is, after the \"Ugly Duckling\", the second picture that really turned me upside down.'],\n",
              "       ['I am a big fan of Ludlum\\'s work, and of the Covert-one books, and I had often thought how incredible they would be made into a film. Imagine my excitement, then, on learning that such a movie actually existed! The \\'Hades Factor\\' being the first in the series seemed an obvious place to start.<br /><br />From the outset the film was disappointing. Simple elements from the film such as Griffin\\'s first meeting with Smith are needlessly different from the book, and much less exhilarating. Several characters are poorly cast, too. For starters Dorff is woeful as Smith. Not a bad actor, just an incredibly bad choice as he is far too soft, and fails to exhibit many of the features that are definitive of John Smith.<br /><br />Re-naming, re-assignment and even omission of certain characters further degrades this film. For example the removal of Victor Tremont and the entire back-story of the virus, including the involvement of VAXHAM makes the entire point to the film somewhat hazy. Marty Zellerbach is a very large part of the book, and in the seat he takes vary much a back seat (not to mention that the film character shares nothing in common with the character in the book) is another big mistake.<br /><br />Rachel Russel is presumably supposed to be Randi Russel from the book. Not only is she supposed to be the sister of Sophie Amsden (should be called Sophia Russel) but she is also supposed to work from the CIA, NOT \"Covert-one\". Which brings me to my'],\n",
              "       [\"How can a movie be both controversial and gentle? This one does it with a near-perfect structure. No one wants their daughters to be athletes. Apparently most cultures don't want their daughters to be small-breasted, either. Here we see a bunch of superb actors we've never heard of before portray folks of different cultures living fairly humdrum lives until their female children want to, and have the potential to, become professional soccer players. The structure around the parallelism of the two cultures is wonderful. There is no condescension. Both cultures are seen as modern and valid. (And yes, both are silly, too). One flaw: the Hindu wedding ceremony seemed to involve hundreds of relatives but not one child among them.\"],\n",
              "       [\"OMG this is one of the worst films iv ever seen and iv seen a lot I'm a Film student. I don't understand why Angelina Jolie would be in this movie? Did she need the money that badly? I love AJ and have seen almost everything shes ever been in so i watched this 2 tick another one off. It was SOO bad! not even good bad, just bad bad. It had 1 or 2 funny little moments but all in all it was bad n a waste of 101 minutes. I cant even say AJ looked good in it because well she didn't. The plot is predictable unless you r expecting a re-telling of Romeo and Juliet then its not. All round disappointing. Maybe if your 12 this could be a good film otherwise I really don't recommend it.\"],\n",
              "       [\"Although I can understand the bad things someone has to say about this movie, I still found it to be absolutely amazing. It will touch you, and unless your a critic searching deep into the flaws and mishaps of every movie, or you just simply aren't touched by anything, it is worth seeing. Don't come into the movie expecting anything, just have a box of tissues and an open mind. It is beautiful and the acting is brilliant. I think Will Smith, despite that he's yet again playing another lonely depressed individual, is amazing. I believe a good actor is someone who can truly portray feelings and emotions we all have at our worst/best experiences in such a way that it reaches out to you and makes YOU feel something. And that's exactly what this movie does. Give it a chance.\"],\n",
              "       [\"There isn't a whole lot going on in this story -- just two men employing very different ways of handling memories of Vietnam. But what it lacks in premise, it more than makes up for in acting and realism. It's a quiet film about the bonds of friendship and shared experience. We even get romance (not gratuitous -- just another very real piece of this story). It's well worth seeing.\"],\n",
              "       ['There are some extremely talented black directors Spike Lee,Carl Franklin,Billy Dukes,Denzel and a host of others who bring well deserved credit to the film industry . Then there are the Wayans Brothers who at one time(15,years ago) had an extremely funny television show\\'In Living Colour\\' that launched the career of Jim Carrey amongst others . Now we have stupidity substituting for humour and gross out gags(toilet humour) as the standard operating procedure . People are not as stupid as those portrayed in \\'Little Man\\' they couldn\\'t possibly be . A baby with a full set of teeth and a tattoo is accepted as being only months old ? Baby comes with a five o\\'clock shadow that he shaves off . It is intimated that the baby has sex with his foster mother behind her husbands,Darryl\\'s, back .Oh, yea that is just hilarious . As a master criminal \\'Little Man\\' is the stupidest on planet earth . He stashes a stolen rock that is just huge in a woman\\'s purse and then has to pursue her . Co-star Chazz Palminteri,why Chazz, offers the best line: \"I\\'m surrounded by morons.\" Based, without credit, on a Chuck Jones cartoon, Baby Buggy Bunny . This is far too stupid to be even remotely funny . A clue as to how bad this film is Damon Wayans appeared on Jay Leno the other night,prior to the BAT awards and he did not,even mention this dreadful movie . When will Hollywood stop green lighting trash from the Wayans Brothers . When they'],\n",
              "       [\"Ok, first of all, I am a huge zombie movie fan. I loved all of Romero's flicks and thoroughly enjoyed the re-make of Dawn of the Dead. So when I had heard every single critic railing this movie I was still optimistic. I mean, critics hated Resident Evil, and while it may not be a particularly great film, I enjoyed it if not for the fact that it was just a fun zombie shoot-em up with a half decent plot. This however, is pure crap. Terrible dialogue, half-assed plot, and video game scenes inserted into the film. Who in their right mind thought that was a good idea. The only thing about this movie (I use the term loosely) that I enjoyed was Jurgen Prochnow as Captain Kirk (Ugh). While his name throws originality out the window, you can see in his performance that he knows he's in a god awful film and he might as well make the best of it. Everyone else acts as if they're doing Shakespeare. And very badly I might add. Basically the only reason anyone should see this monstrosity is if you a.) Are a huge zombie buff and must see every zombie flick made or b.) Like to play MST3K, the home game. See it with friends and be prepared for tons of unintentional laughs.<br /><br />\"],\n",
              "       ['\" It had to be You\" is another sign that Hollywood is running out of ideas. This picture is about Charlie Hudson a former police officer turned Author. When Charlie\\'s fiancé goes out of town he\\'s stuck with all of the wedding planning. He spends a week at a fancy hotel and meets Anna Penn a teacher who just happens to also be getting married. The two quickly become friends and set out to plan their separate weddings together. This is when the plot gets boring, Charlie falls in love with Anna and she has to choose between a safe life or Charlie. This movie rips off every romantic comedy ever made and just has you waiting for the end of the movie so you can do something else. Micahel Vartan and Natasha Henstridge give really mediocre performances which just makes this movie all the more gut wrenching to watch.'],\n",
              "       ['The title above is used to introduce the film \"Gen\" to its audience. Gen is about a young doctor(Doga Rutkay) with an ill mother. The film starts with her leaving her mother behind to start her new job. While she drives, we realise that she is not that close home as the hospital is in a remote area. As soon as she steps into the garden of the hospital, she sees the death body of a patient. This is the beginning of a nightmare for the next few days.<br /><br />Two policemen comes to the hospital so as to investigate the suicide. In fact, they will have to stay in the hospital because all roads are cut off due to bad weather conditions. All their communication with outside world is cut off too. There is no way out!! In those few days, there will be more nasty murders. Now everybody suspects from each other.<br /><br />In my opinion, the idea is brilliant. It could have been very scary indeed. There are positive sides of the movie of course. I really like the beginning of the movie. Especially, when she drives to the hospital and her first moments in the hospital. Actings are okay. Some of them are trying too hard to be mysterious and scary though. I think the final shock should have been spread into the through out of the movie. What I am saying is, it was a good twist but instead of showing it as a parody in the end, we should have']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sANyFHt0y2HQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a216355-21d1-4100-ac62-32c6be64c83b"
      },
      "source": [
        "train_label[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 1, 0, 1, 1, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TPE_0jfm4pg",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize\n",
        "create input_ids, input_masks, and segment_ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeKIXphYbQmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "14bffcb9-7745-4d69-e0d8-58879cfe6b9d"
      },
      "source": [
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        np.array(labels).reshape(-1, 1),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
        "        )\n",
        "    return InputExamples\n",
        "\n",
        "# Instantiate tokenizer\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "# Convert data to InputExample format\n",
        "train_examples = convert_text_to_examples(train_text, train_label)\n",
        "test_examples = convert_text_to_examples(test_text, test_label)\n",
        "\n",
        "# Convert to features\n",
        "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
        ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
        "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
        ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb6ee335daec42cfaaf49dd429257080",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25000, style=ProgressSt…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f860b506673f485893d6e9eee8ce981e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25000, style=ProgressSt…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et7VhP_AbQms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fine_tune_layers=10,\n",
        "        pooling=\"first\",\n",
        "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "        if self.pooling not in [\"first\", \"mean\"]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
        "        )\n",
        "\n",
        "        # Remove unused layers\n",
        "        trainable_vars = self.bert.variables\n",
        "        if self.pooling == \"first\":\n",
        "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "            trainable_layers = [\"pooler/dense\"]\n",
        "\n",
        "        elif self.pooling == \"mean\":\n",
        "            trainable_vars = [\n",
        "                var\n",
        "                for var in trainable_vars\n",
        "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
        "            ]\n",
        "            trainable_layers = []\n",
        "        else:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        for i in range(self.n_fine_tune_layers):\n",
        "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
        "\n",
        "        # Update trainable vars to contain only the specified layers\n",
        "        trainable_vars = [\n",
        "            var\n",
        "            for var in trainable_vars\n",
        "            if any([l in var.name for l in trainable_layers])\n",
        "        ]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        if self.pooling == \"first\":\n",
        "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"pooled_output\"\n",
        "            ]\n",
        "        elif self.pooling == \"mean\":\n",
        "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"sequence_output\"\n",
        "            ]\n",
        "\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            pooled = masked_reduce_mean(result, input_mask)\n",
        "        else:\n",
        "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4snZoaO-bQmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "def build_model(max_seq_length): \n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
        "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eokfIybLbQmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "f19f34e1-4bf3-4073-d569-3134a98bcf8d"
      },
      "source": [
        "model = build_model(max_seq_length)\n",
        "\n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n",
        "\n",
        "model.fit(\n",
        "    [train_input_ids, train_input_masks, train_segment_ids], \n",
        "    train_labels,\n",
        "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
        "    epochs=1,\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 110,302,011\n",
            "Trainable params: 22,051,329\n",
            "Non-trainable params: 88,250,682\n",
            "__________________________________________________________________________________________________\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "25000/25000 [==============================] - 1201s 48ms/sample - loss: 0.7500 - acc: 0.5057 - val_loss: 0.6985 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0cc7255470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_1t7EBobQmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "56fa413a-7270-4ec4-abd0-d4e2c46e3870"
      },
      "source": [
        "model.save('BertModel.h5')\n",
        "pre_save_preds = model.predict([test_input_ids[0:100], \n",
        "                                test_input_masks[0:100], \n",
        "                                test_segment_ids[0:100]]\n",
        "                              ) # predictions before we clear and reload model\n",
        "\n",
        "# Clear and load model\n",
        "model = None\n",
        "model = build_model(max_seq_length)\n",
        "initialize_vars(sess)\n",
        "model.load_weights('BertModel.h5')\n",
        "\n",
        "post_save_preds = model.predict([test_input_ids[0:100], \n",
        "                                test_input_masks[0:100], \n",
        "                                test_segment_ids[0:100]]\n",
        "                              ) # predictions after we clear and reload model\n",
        "all(pre_save_preds == post_save_preds) # Are they the same?"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer_1 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          196864      bert_layer_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            257         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 110,302,011\n",
            "Trainable params: 22,051,329\n",
            "Non-trainable params: 88,250,682\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9woYDpYwbQm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}